{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SDOBenchmark Dataset\n",
    "\n",
    "## Data Exploration \n",
    "\n",
    "### Dataset in a Nutshell\n",
    "\n",
    "Lightweight dataset consisting of patches of SDO data consisting of patches centered around the active regions - some of which lead to flares. Prepared by people from FHNW/I4DS.<br>\n",
    "\n",
    "Algorithm used to create the dataset from the raw data: see https://github.com/i4Ds/SDOBenchmark/blob/master/STRUCTURE.md <br>\n",
    "- The target active region coordinates identified from TODO, calculated on the current image\n",
    "- 512 x 512 patch cut out around the rotated coordinates (TODO??)\n",
    "- Resulting image data clipped by predefined clipping ranges, scaled down to 256x256.\n",
    "- Result saved to disk as 8-bit JPEG.\n",
    "\n",
    "Samples consist of \n",
    "- 4 time steps with \n",
    "- 10 images per time step - 8 images from AIA, 2 images from HMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "Can be downloaded from https://i4ds.github.io/SDOBenchmark/ <br>\n",
    "\n",
    "Two versions:\n",
    "- data-full:    ~3730MB on disk (extracted)\n",
    "- data-example:  ~340MB on disk (extracted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "!curl -L https://github.com/i4Ds/SDOBenchmark/archive/data-example.zip > data-example.zip\n",
    "!unzip -q -o data-example.zip -d ./data\n",
    "!rm -f data-example.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies\n",
    "\n",
    "- on Google Colab you will need to restart your runtime after this step\n",
    "- when running this locally, make sure to first follow the setup instructions in the [README](../README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tqdm\n",
    "!pip install -U torch torchvision\n",
    "!pip install -U numpy\n",
    "!pip install -U matplotlib \n",
    "!pip install -U Pillow\n",
    "!pip install -U pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data location\n",
    "\n",
    "DATADIR = \"./data/SDOBenchmark-data-example\"\n",
    "\n",
    "\n",
    "\n",
    "# download and extract it to DATADIR\n",
    "\n",
    "# location of data\n",
    "data_dir_train = \"%s/training\"%DATADIR\n",
    "data_dir_test  = \"%s/test\"%DATADIR\n",
    "\n",
    "#location of metadata\n",
    "df_index_train = \"%s/training/meta_data.csv\"%DATADIR\n",
    "df_index_test  = \"%s/test/meta_data.csv\"%DATADIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Structure\n",
    "\n",
    "- Split into train and test, accordingly you find a $train$- and $test$-folder (8'336 train, 886 test samples).\n",
    "- Different samples organized in separate subgroups, within a subgroup you will see samples observed close in time (distributed over several days):\n",
    "    - $\\verb|<subgroupid>|$\n",
    "    - within the subgroup folder the data for __one sample__ can be found in a folder $\\verb|<sampleid>|$ named with a date format: $\\verb|YYYY-MM-DD-HH-mm-ss-v|$ (v: version ?? TODO)\n",
    "- Each sample folder contains 40 images - 10 images for each of the 4 time steps.\n",
    "- Images are named composed of a timestamp followed by the AIA band or HMI specification: $\\verb|<timestamp>_<instrument_specifier>.jpg|$\n",
    "    - timestamp: $\\verb|YYYY\\_MMTDDHHmmss|$\n",
    "    - instrument specifier: \n",
    "        - for AIA: $\\verb|94, 131, 171, 193, 211, 304, 335, 1700|$\n",
    "        - for HMI: $\\verb|continuum, magnetogram|$\n",
    "- Label information provided in form of peak flux values:\n",
    "    - one line per sample with $id$ ($\\verb|<subgroupid>_<sampleid>|$), time range the images refer to ($start$, $end$) and $\\verb|peak_flux|$ for the projection period.\n",
    "    - the peak flux can be translated to a flare class label by comparing it with a threshold (for flare classes $\\ge$ C we would use a threshold 1.0e-6, for flare classes $\\ge$ M a threshold 1.0e-5). \n",
    "\n",
    "_Remark_: When splitting train into train and validate it is recommended to split along the subgroups, i.e. not split up the subgroups. \n",
    "\n",
    "The thresholding drives the distribution of the flare classes. When referring to FLARE as at least class C flare, we obtain \n",
    "- train: 346 flare and 7051 non-flare samples (TODO: these numbers are inconsistent) \n",
    "- test:  157 flare and  673 non-flare samples\n",
    "\n",
    "\n",
    "#### Missing Data\n",
    "\n",
    "Not for all samples we have data for all the 10 channels. This should be considered when loading and possibly processing and modeling the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "AIA_94, AIA_131, AIA_171, AIA_193, AIA_211, AIA_304, AIA_335, AIA_1700 = \"94\", \"131\", \"171\", \"193\", \"211\", \"304\", \"335\", \"1700\"\n",
    "HMI_CONT, HMI_MAGN = \"continuum\", \"magnetogram\"\n",
    "ALL_CHANNELS = [AIA_94, AIA_131, AIA_171, AIA_193, AIA_211, AIA_304, AIA_335, AIA_1700, HMI_CONT, HMI_MAGN]\n",
    "\n",
    "TIMESTEPS = [timedelta(), timedelta(hours=7), timedelta(hours=10, minutes=30), timedelta(hours=11, minutes=50)]\n",
    "\n",
    "DATEFORMAT_IMGNAME = \"%Y-%m-%dT%H%M%S\"\n",
    "\n",
    "class SDOBenchmarkIndex():\n",
    "    \n",
    "    def __init__(self, data_dir, df_index, channels=None, threshold=10e-6):\n",
    "        self.data_dir = data_dir\n",
    "        self.df_index = pd.read_csv(df_index)\n",
    "        self.channels = self.channels_to_check(channels)\n",
    "        self.threshold = threshold\n",
    "        self.sample_dirs = []\n",
    "        self.image_files = {}\n",
    "        self.labels = {}\n",
    "\n",
    "        print(\"Checking for channels: %s\"%', '.join([ch for ch in self.channels]))\n",
    "        print(\"Using threshold      : %9.8f\"%self.threshold)\n",
    "        \n",
    "        for index, row in self.df_index.iterrows():\n",
    "            groupid = row['id'][:5]\n",
    "            dateid = row['id'][6:]\n",
    "            group_dir = os.path.join(data_dir,groupid )\n",
    "            sample_dir = os.path.join(group_dir, dateid)\n",
    "            start_date = parser.parse(row['start'])\n",
    "\n",
    "            if self.check_sample_dir(sample_dir, start_date, self.channels):\n",
    "                imgs_steps = {}\n",
    "                self.sample_dirs.append(sample_dir)\n",
    "                self.labels[sample_dir] = self.create_binary_label(row['peak_flux'])\n",
    "                for i in range(len(TIMESTEPS)):\n",
    "                    imgs_channels = {}\n",
    "                    for ch in self.channels:\n",
    "                        _name = self.create_image_name(start_date, TIMESTEPS[i], ch) \n",
    "                        imgs_channels[ch] = os.path.join(sample_dir, _name)\n",
    "                    imgs_steps[i] = imgs_channels\n",
    "            \n",
    "                self.image_files[sample_dir]=imgs_steps\n",
    "\n",
    "        print(\"Found %i samples from which %i are flares.\"%(len(self.sample_dirs), sum(self.labels.values())))\n",
    "\n",
    "    def channels_to_check(self, channels):\n",
    "        if not channels:\n",
    "            channels = ALL_CHANNELS.copy()\n",
    "        return channels\n",
    "\n",
    "    def create_image_name(self, base_date, delta, channel):\n",
    "        img_date = base_date+delta\n",
    "        img_name = \"%s__%s.jpg\"%(datetime.strftime(img_date, DATEFORMAT_IMGNAME),channel)\n",
    "        return img_name\n",
    "    \n",
    "    def check_sample_dir(self, sample_dir, start, channels): #check if the directory contains all samples (no missing)        \n",
    "        for delta in TIMESTEPS:\n",
    "            for ch in channels:\n",
    "                _name = self.create_image_name(start, delta, ch)\n",
    "                _path = os.path.join(sample_dir, _name)\n",
    "                if not os.path.exists(_path):\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def create_binary_label(self, val):\n",
    "        return int(val>self.threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_index = SDOBenchmarkIndex(data_dir_train, df_index_train, channels=(HMI_MAGN,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Glances at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = train_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.choice(list(data.image_files.keys()))\n",
    "channel = HMI_MAGN\n",
    "timestep = 0\n",
    "\n",
    "image_path = data.image_files[sample][timestep][channel]\n",
    "image = Image.open(image_path)\n",
    "imgarray = np.asarray(image)\n",
    "\n",
    "print(image.size, image.mode)\n",
    "print(imgarray.min(), imgarray.max())\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 4\n",
    "channel = HMI_MAGN\n",
    "\n",
    "samples = [np.random.choice(list(data.image_files.keys())) for i in range(nsamples)]\n",
    "nsteps = 4\n",
    "\n",
    "plt.figure( figsize=(nsteps*3,nsamples*3) )\n",
    "for i in range(nsamples):\n",
    "    for j in range(nsteps):\n",
    "        plt.subplot( nsamples, nsteps, nsamples*i+j+1 )\n",
    "        image_file = data.image_files[samples[i]][j][channel]\n",
    "        image = plt.imread(image_file)\n",
    "        plt.title( \"Sample %i, Step %i\"%(i,j))\n",
    "        plt.imshow( image[:,:], cmap=\"gray\" )\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Large Mosaic (Timestep 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "channel = HMI_MAGN\n",
    "timestep = 0\n",
    "\n",
    "plt.figure( figsize=(n*2,n*2) )\n",
    "samples = [np.random.choice(list(data.image_files.keys())) for i in range(n*n)]\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        plt.subplot( n, n, n*i+j+1 )\n",
    "        image_file = data.image_files[samples[n*i+j]][timestep][channel]\n",
    "        image = plt.imread(image_file)\n",
    "        plt.title( \"Sample %i\"%(i))\n",
    "        plt.imshow( image[:,:], cmap=\"gray\" )\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flux Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist, bins, _ = plt.hist(data.df_index.peak_flux, bins=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Data Loader (Dataset)\n",
    "\n",
    "using SDOBenchmarkIndex \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDOBenchmarkDataset(SDOBenchmarkIndex, nn.Module):\n",
    "    def __init__(self, data_dir, df_index, channels=None, threshold=10e-6, size=256):\n",
    "        super().__init__( data_dir, df_index, channels=None, threshold=10e-6)\n",
    "        self.mean=0.4997\n",
    "        self.std=0.1004\n",
    "        self.size=size\n",
    "\n",
    "    def transform (self, img_list):\n",
    "        to_ten=transforms.ToTensor()\n",
    "        for idx, img in enumerate(img_list):\n",
    "            img_list[idx] = to_ten(img)\n",
    "        \n",
    "        resize=transforms.Resize(self.size)\n",
    "        for idx, img in enumerate(img_list):\n",
    "            img_list[idx]=resize(img)\n",
    "\n",
    "        return img_list\n",
    "        \n",
    "    def create_channels(self, fold):\n",
    "        l_all=[]\n",
    "        for i in fold.keys():\n",
    "            imgg=Image.open(fold[i]['magnetogram'])\n",
    "            l_all.append(imgg)\n",
    "        l_all =self.transform(l_all)\n",
    "        for idx, img in enumerate(l_all):\n",
    "            l_all[idx] = img.squeeze()\n",
    "        return torch.stack(l_all)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        fold=self.image_files[self.sample_dirs[index]]\n",
    "        res_all=self.create_channels(fold)\n",
    "        return res_all, self.labels[self.sample_dirs[index]]\n",
    "       \n",
    "    def __len__(self):\n",
    "    \n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = SDOBenchmarkDataset(data_dir_train, df_index_train, channels=(HMI_MAGN,), size=64)\n",
    "test_index = SDOBenchmarkDataset(data_dir_test, df_index_test, channels=(HMI_MAGN,), size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 channel CNN that output 2 probabilities for flare and for non-flare\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim, nf):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        colors_dim=4\n",
    "        \n",
    "        self.cnn_layers=nn.Sequential(\n",
    "            #input is (nc)\n",
    "            nn.Conv2d(colors_dim, nf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            # state size: (nf) *32 *32\n",
    "            nn.Conv2d(nf, nf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(nf * 2),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            # state size: (nf * 4) * 16 * 16\n",
    "            nn.Conv2d(nf * 2, nf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(nf * 4),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            # state size: (nf * 4) * 8* 8)\n",
    "        )\n",
    "        self.linear1=nn.Linear(32*4*8*8, 32*4*2)\n",
    "        self.linear2=nn.Linear(32*4*2, 2)\n",
    "        \n",
    "        \n",
    "    def forward (self, x):\n",
    "        x=self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x=self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x=self.linear2(x)\n",
    "        x=F.softmax(x, dim=1)\n",
    "        \n",
    "        return x.squeeze()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "#creating dataloders \n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(train_index, batch_size=64, pin_memory=True)\n",
    "test_loader=torch.utils.data.DataLoader(test_index, batch_size=64)\n",
    "\n",
    "\n",
    "#creating weights\n",
    "cl_weights=class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(list(train_index.labels.values())), y=list(train_index.labels.values()))\n",
    "cl_weights=torch.Tensor(cl_weights)\n",
    "\n",
    "# using cuda if available for training and testing\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#weight to device\n",
    "cl_weights=cl_weights.to(device)\n",
    "\n",
    "\n",
    "#creating the model\n",
    "\n",
    "\n",
    "z_dim=2\n",
    "nf=32\n",
    "model=CNN(z_dim, nf)\n",
    "print(model)\n",
    "#model to device\n",
    "model.to(device)\n",
    "\n",
    "#creating opimizer\n",
    "lr=0.0001\n",
    "optim=torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "#creating loss function\n",
    "loss=nn.CrossEntropyLoss(cl_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ar=[]\n",
    "loss_test_ar=[]\n",
    "\n",
    "y_out=torch.Tensor([])\n",
    "y_all=torch.Tensor([])\n",
    "\n",
    "y_out_test=torch.Tensor([])\n",
    "y_all_test=torch.Tensor([])\n",
    "\n",
    "epoch=10\n",
    "for epochs in range (epoch):\n",
    "    \n",
    "    #creating sum of losses\n",
    "    loss_all=0\n",
    "    loss_all_test=0\n",
    "    \n",
    "    count=0\n",
    "    count_test=0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        model.train()\n",
    "        \n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        if epochs==epoch-1:\n",
    "            y_all=torch.cat([y_all, y])\n",
    "        \n",
    "        #calculating output\n",
    "        output=model(x)\n",
    "        if epochs==epoch-1:\n",
    "            y_out=torch.cat([y_out, output])\n",
    "        \n",
    "        #calculating loss\n",
    "        l=loss(output, y.long())\n",
    "        \n",
    "        #backprop\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        # adding losses\n",
    "        loss_all=loss_all + l\n",
    "        count+=1\n",
    "        \n",
    "   \n",
    "    \n",
    "    #calculating output for test dataset\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "           \n",
    "            #data to device\n",
    "            \n",
    "            x=x.to(device)\n",
    "            y=y.to(device)\n",
    "            if epochs==epoch-1:\n",
    "                y_all_test=torch.cat([y_all_test, y])\n",
    "        \n",
    "            #calculating output\n",
    "            \n",
    "            output=model(x)\n",
    "            if epochs==epoch-1:\n",
    "                y_out_test=torch.cat([y_out_test, output])\n",
    "        \n",
    "            \n",
    "            l=loss(output, y.long())\n",
    "            \n",
    "            #adding losses\n",
    "            loss_all_test=loss_all_test + l\n",
    "            count_test+=1\n",
    "            \n",
    "            \n",
    "    #calculating losses\n",
    "    loss_tr=float(loss_all)/count\n",
    "    loss_ts=float(loss_all_test)/count_test\n",
    "    \n",
    "    loss_ar.append(loss_tr)\n",
    "    loss_test_ar.append(loss_ts)\n",
    "    \n",
    "    print('epoch: {}, train_loss: {}, test_loss: {} \\n'.format(epochs, loss_tr, loss_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting learning curves\n",
    "\n",
    "epochs=range(1, len(loss_ar)+1)\n",
    "plt.plot(epochs, loss_ar, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_test_ar, 'b', label='Test loss')\n",
    "plt.title('training and test loss ')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tresh = 0.5 \n",
    "\n",
    "cmat_test = confusion_matrix(y_all_test, np.array([y_out_test[i][0]<tresh for i in range(len(y_out_test))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tss(cmat):\n",
    "    \n",
    "    tp=cmat[1][1]\n",
    "    fn=cmat[1][0]\n",
    "    fp=cmat[0][1]\n",
    "    tn=cmat[0][0]\n",
    "    \n",
    "    eps=1e-6\n",
    "    \n",
    "    tss=round(float(tp)/float(tp+fn+eps)-float(fp)/float(fp+tn+eps), 3)\n",
    "    \n",
    "    return tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cmat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tss: {}'.format(tss(cmat_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
